{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the Nyl documentation","text":"<p>Nyl is a flexible configuration management tool for Kubernetes resources that can be used to generate and deploy applications directly or integrate as an ArgoCD ConfigManagementPlugin.</p>"},{"location":"reference/applysets/","title":"ApplySets","text":"<p>Kubernetes ApplySets are a method of managing groups of Kubernetes resources for safely applying and pruning resources in a cluster. Nyl provides basic support for ApplySets, allowing you to keep track of deployed resources and prune resources as they are removed from your configuration.</p> <p>Note that ApplySet support is experimental and has still has various issues. You can following the progress of the feature in the nyl#5.</p>"},{"location":"reference/argocd-plugin/","title":"ArgoCD Plugin","text":"<p>This page describes Nyl's integration as an ArgoCD ConfigManagementPlugin.</p>"},{"location":"reference/argocd-plugin/#installation","title":"Installation","text":"<p>Config management plugins are installed as additional containers to the <code>argocd-repo-server</code> Pod. They launch the <code>argocd-cmp-server</code> binary and communicates with ArgoCD over gRPC via a socket file shared between the repo-server and the plugin container under <code>/home/argocd/cmp-server/plugins</code>.</p> <p>We recommend the following configuration:</p> argocd-values.yaml<pre><code>repoServer:\n  extraContainers:\n    - name: nyl-v1\n      image: ghcr.io/niklasrosenstein/nyl/argocd-cmp:{{ NYL_VERSION }}\n      securityContext:\n        runAsNonRoot: true\n        runAsUser: 999\n      volumeMounts:\n        - mountPath: /var/run/argocd\n          name: var-files\n        - mountPath: /home/argocd/cmp-server/plugins\n          name: plugins\n        - mountPath: /tmp\n          name: cmp-tmp\n      envFrom:\n        - secretRef:\n            name: argocd-nyl-env\n      env:\n        - name: NYL_CACHE_DIR\n          value: /tmp/nyl-cache\n        - name: NYL_LOG_LEVEL\n          value: info\n  clusterRoleRules:\n    enabled: true\n  volumes:\n    - name: cmp-tmp\n      emptyDir: {}\n</code></pre> <p>Warning</p> <p>The <code>clusterRoleRules.enabled=true</code> option enables the plugin to access the Kubernetes API. This is necessary for various Nyl features to function correctly (such as lookups, see Cluster connectivity). If you do not wish to grant the plugin access to the Kubernetes API, you must disable this option and ensure that your manifests do not rely on features that require API access.</p>"},{"location":"reference/argocd-plugin/#discovery","title":"Discovery","text":"<p>The ArgoCD plugin runs <code>nyl argocd discovery</code> as the discovery command to determine if a repository is compatible with Nyl. This means that if you create an ArgoCD application that points to a Git repository with at least one Nyl configuration file in it, the plugin will be invoked implicitly without specifying the <code>nyl-v1</code> plugin name in the application spec.</p>"},{"location":"reference/argocd-plugin/#one-file-per-application","title":"One file per application","text":"<p>ArgoCD applications do not permit to point their <code>source.path</code> field to a file within a repository, it must be a directory. For this, Nyl accepts a <code>ARGOCD_ENV_NYL_CMP_TEMPLATE_INPUT</code> environment variable that can be a comma-separate list of filenames that you would pass to <code>nyl template</code> as arguments. Nyl will then ignore the default <code>.</code> argument (pointing to the current directory, which is the directory specified with <code>source.path</code>) and use the files specified via the environment variable instead.</p> argocd-application.yaml<pre><code># trimmed example\nspec:\n  source:\n    plugin:\n      name: nyl-v1\n      env:\n        - name: NYL_CMP_TEMPLATE_INPUT\n          value: '{{.path.filename}}'\n</code></pre>"},{"location":"reference/argocd-plugin/#applicationset-example","title":"ApplicationSet example","text":"<p>A desirable pattern for using Nyl with ArgoCD is to create on application per YAML file in the directory corresponding to your cluster in the repository. The following example demonstrates how to setup an ArgoCD <code>ApplicationSet</code> that does exactly this:</p> appset.yaml<pre><code>apiVersion: argoproj.io/v1alpha1\nkind: ApplicationSet\nmetadata:\n  name: main\n  namespace: argocd\nspec:\n  goTemplate: true\n  goTemplateOptions: [\"missingkey=error\"]\n  generators:\n  - git:\n      repoURL: git@github.com:myorg/gitops.git\n      revision: HEAD\n      files:\n        - path: \"clusters/my-cluster/*.yaml\"\n  template:\n    metadata:\n      name: '{{.path.filename | trimSuffix \".yaml\" | slugify }}'\n    spec:\n      project: default\n      source:\n        repoURL: git@github.com:myorg/gitops.git\n        targetRevision: HEAD\n        path: '{{.path.path}}'\n        plugin:\n          name: nyl-v1\n          env:\n            - name: NYL_CMP_TEMPLATE_INPUT\n              value: '{{.path.filename}}'\n      destination:\n        server: https://kubernetes.default.svc\n        namespace: '{{.path.basename}}'\n      syncPolicy:\n        syncOptions:\n          - CreateNamespace=true\n          - ServerSideApply=true\n</code></pre> <p>You may treat <code>appset.yaml</code> as a member of the same directory, allowing it to be managed by its own \"appset\" application created by the <code>ApplicationSet</code> itself.</p> <p>Note that in order for the <code>argocd-applicationset-controller</code> to be able to clone your Git repository via SSH, you need to configure a Credential template that matches the <code>spec.generators[0].git.repoURL</code> field, whereas for the individual applications to clone the repository you need to configure a Repository.</p> repository-secrets.yaml<pre><code># For the ApplicationSet\n---\nkind: Secret\nmetadata:\n  name: github-creds\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repo-creds\ntype: Opaque\nstringData:\n  project: default\n  name: github.com\n  url: git@github.com:myorg/\n  type: git\n  sshPrivateKey: ...\n\n# For the Application(s), but credentials can be omitted as they are inherited from the repo-creds above.\n---\nkind: Secret\nmetadata:\n  name: github-repo-gitops\n  namespace: argocd\n  labels:\n    argocd.argoproj.io/secret-type: repository\ntype: Opaque\nstringData:\n  project: default\n  type: git\n  url: git@github.com:myorg/gitops.git\n</code></pre>"},{"location":"reference/argocd-plugin/#debugging-the-plugin","title":"Debugging the plugin","text":"<p>The ArgoCD plugin produces per-project/application logs in the <code>/var/log</code> directory of the <code>nyl-v1</code> container in the <code>argocd-repo-server</code> pod. These logs are often much easier to inspect than the output the template rendering fails and ArgoCD reports stderr to the UI.</p> <p>At the start of each invokation of Nyl, the command will debug-log some useful basic information:</p> <ul> <li>The command-line used to invoke Nyl.</li> <li>The current working directory.</li> <li>All Nyl-relevant environment variables (such that start with <code>ARGOCD_</code>, <code>NYL_</code> and <code>KUBE_</code>).</li> </ul> <p>At the end Nyl will also print the command-line again as well as the time it took for the command to complete. Note that in order to see these logs you should set the <code>NYL_LOG_LEVEL</code> environment variable to <code>debug</code>.</p>"},{"location":"reference/cluster-connectivity/","title":"Cluster connectivity","text":"<p>Nyl may need to reach out to the Kubernetes API for various reasons, some of which are fundamental and others are optional.</p> <p>When using Nyl as an ArgoCD plugin, to enable the plugin to reach out to the Kubernetes API, you must configure the <code>argocd-repo-server</code> service account with the necessary permissions. See ArgoCD Plugin for more information.</p>"},{"location":"reference/cluster-connectivity/#kubernetes-api-versions","title":"Kubernetes API versions","text":"<p>When Nyl invokes <code>helm template</code>, it must pass along a full list of all available API versions in the cluster to allow the chart to generate appropriate manifests for all the latest resources it supports via the <code>--api-versions</code> and <code>--kube-version</code> flags.</p> <p>Note that when used from ArgoCD, the <code>KUBE_VERSION</code> and <code>KUBE_API_VERSIONS</code> environment variables are set by ArgoCD and Nyl will use them if available to avoid making an extra query to the Kubernetes API server. For more information, see ArgoCD Build Environment.</p>"},{"location":"reference/cluster-connectivity/#lookups","title":"Lookups","text":"<p>Nyl provides a <code>lookup()</code> function that allows the Helm chart to query the Kubernetes API server for an existing resource to use in the chart. This is an optional feature that your manifests may simply decide not to rely on, however it is a powerful feature to pass and transform values from existing resources.</p> <p>TODO: Implement security to prevent lookups for resources that the corresponding ArgoCD project has no access to. This will require a safe evaluation language instead of Python <code>eval()</code>.</p>"},{"location":"reference/glossary/","title":"Glossary","text":""},{"location":"reference/glossary/#manifest","title":"Manifest","text":"<p>A manifest is a file that may define zero or more Kubernetes or Nyl inline resources. Nyl understands individual manifests, i.e. YAML files, and certain behaviours operate on that level, such as Kubernetes Namespace auto-filling and resource post-processing applying only to all resources defined in the same manifest as the <code>PostProcessor</code>.</p>"},{"location":"reference/glossary/#profile","title":"Profile","text":"<p>...</p>"},{"location":"reference/glossary/#project","title":"Project","text":"<p>...</p>"},{"location":"reference/glossary/#resource","title":"Resource","text":"<p>A resource is a YAML document that follows the schema of a Kubernetes API resource or Nyl inline resource. An example would be a Kubernetes <code>ConfigMap</code>, <code>Deployment</code>, <code>Pod</code> or a Nyl <code>HelmChart</code> or <code>PostProcessor</code>.</p>"},{"location":"reference/glossary/#secrets-provider","title":"Secrets provider","text":"<p>...</p>"},{"location":"reference/configuration/environment/","title":"Environment variables","text":"<p>This page summarizes all environment variables that are used by Nyl.</p> <ul> <li><code>NYL_LOG_LEVEL</code> \u2013 The log level to use if <code>--log-level</code> is not specified. Defaults to <code>info</code>. Used by: <code>nyl</code>.</li> <li><code>NYL_PROFILE</code> \u2013 The name of the profile to use as defined in the closest <code>nyl-profiles.yaml</code> or   <code>nyl-project.yaml</code> configuration file. Used by: <code>nyl profile</code>, <code>nyl template</code>, <code>nyl tun</code>.</li> <li><code>NYL_SECRETS</code> \u2013 The name of the secrets provider to use as defined in the closest <code>nyl-secrets.yaml</code> or   <code>nyl-project.yaml</code> configuration file. Used by: <code>nyl secrets</code>, <code>nyl template</code>.</li> <li><code>NYL_STATE_DIR</code> \u2013 The directory where Nyl stores its state, such as current profile data, which may include   fetched Kubeconfig file. Defaults to <code>.nyl</code> relative to the <code>nyl-project.yaml</code> or the current working directory.   Used by: <code>nyl profile</code>, <code>nyl template</code>, <code>nyl tun</code>.</li> <li><code>NYL_CACHE_DIR</code> \u2013 The directory where Nyl stores its cache, such as downloaded Helm charts and cloned   repositories. Defaults to <code>cache/</code> relative to the <code>NYL_STATE_DIR</code>. Used by <code>nyl template</code>.</li> <li><code>ARGOCD_ENV_NYL_CMP_TEMPLATE_INPUT</code> \u2014 This variable is only recognized by <code>nyl template</code> when the only positional argument   it receives is <code>.</code> (i.e. the current working directory). The variable should be a comma-separated list of filenames   that should be treated as if the files were passed as arguments to <code>nyl template</code> instead. This is used for the Nyl   ArgoCD plugin to allow specifying exactly which files should be templated as part of an ArgoCD application.</li> <li><code>KUBE_VERSION</code> \u2013 The version of the Kubernetes cluster. If this is not set, Nyl will try to query the Kubernetes   API server to determine the version. When used as an ArgoCD plugin, this variable is usually available   <sup>1</sup>. Used by: <code>nyl template</code>.</li> <li><code>KUBE_API_VERSIONS</code> \u2013 A comma-separated list of all available API versions in the cluster. If this is not set,   Nyl will try to query the Kubernetes API server to determine the versions. When used as an ArgoCD plugin, this   variable is usually available <sup>1</sup>. Used by: <code>nyl template</code>.</li> </ul> <ol> <li> <p>See ArgoCD Build Environment.\u00a0\u21a9\u21a9</p> </li> </ol>"},{"location":"reference/configuration/profiles/","title":"Profiles","text":"<p>Profiles allow you to tell Nyl how to connect to a Kubernetes cluster. They work in concert with traditional \"kubeconfig\" and allow you to have a certain level of assurance you are targeting the expected cluster, as well as providing additional connection methods (such as via SSH tunnel).</p> <p>ArgoCD</p> <p>Profiles are not required with ArgoCD, as the target cluster is defined in the ArgoCD application. Having a profile configuration in your repository when deploying with ArgoCD may have unintended consequences for the way Nyl interacts with the target cluster.</p>"},{"location":"reference/configuration/profiles/#configuration","title":"Configuration","text":"<p>Profiles are defined in a <code>nyl-profiles.&lt;ext&gt;</code> file that is located in the current working directory or any of its parent directories. Profiles may also be defined in a Project configuration file, though the file closer to the working directory will take precedence.</p> <p>If no profile configuration is found this way, Nyl will look for a global configuration file in <code>~/.nyl/nyl-profiles.&lt;ext&gt;</code>.</p> <p>As with other configuration file types, the file extension can be <code>.toml</code>, <code>.yaml</code>, or <code>.json</code>.</p> <p>The configuration contains any number of named profiles. When not specified otherwise, Nyl will assume that the profile to use is named <code>default</code>. The profile to use can be overriden by setting the <code>NYL_PROFILE</code> environment variable, or by passing the corresponding CLI option to respective Nyl commands.</p>"},{"location":"reference/configuration/profiles/#profile-definition","title":"Profile definition","text":"<p>A profile describes:</p> <ol> <li>How to obtain the kubeconfig for the target cluster.</li> <li>(Optionally) How to connect to the target cluster via a tunnel.</li> </ol> <p>The tunnel configuration is useful when the target cluster is not directly accessible from the machine running Nyl, for example when the cluster is behind a firewall or on-premises but can be accessed via an SSH jump host.</p>"},{"location":"reference/configuration/profiles/#example","title":"Example","text":"<p>The following example has Nyl fetch the kubeconfig from a remote machine via SSH, then open an SSH tunnel to the same machine and use that to connect to the Kubernetes cluster.</p> TOMLYAMLJSON nyl-profile.toml<pre><code>[default.kubeconfig]\ntype = \"ssh\"\nuser = \"root\"\nhost = \"mycluster.example.com\"\npath = \"/etc/rancher/k3s/k3s.yaml\"\n\n[default.tunnel]\ntype = \"ssh\"\nuser = \"root\"\nhost = \"mycluster.example.com\"\n</code></pre> nyl-profile.yaml<pre><code>default:\n  kubeconfig:\n    type: ssh\n    user: root\n    host: mycluster.example.com\n    path: /etc/rancher/k3s/k3s.yaml\n  tunnel:\n    type: ssh\n    user: root\n    host: mycluster.example.com\n</code></pre> nyl-profile.json<pre><code>{\n  \"default\": {\n    \"kubeconfig\": {\n      \"type\": \"ssh\",\n      \"user\": \"root\",\n      \"host\": \"mycluster.example.com\",\n      \"path\": \"/etc/rancher/k3s/k3s.yaml\"\n    },\n    \"tunnel\": {\n      \"type\": \"ssh\",\n      \"user\": \"root\",\n      \"host\": \"mycluster.example.com\"\n    }\n}\n</code></pre> <p>In the following example, the kubeconfig is also fetched via SSH, but the server hostname is replaced with one that is reachable from your local machine.</p> TOMLYAMLJSON nyl-profile.toml<pre><code>[default.kubeconfig]\ntype = \"ssh\"\nuser = \"root\"\nhost = \"mycluster.example.com\"\npath = \"/etc/rancher/k3s/k3s.yaml\"\nreplace_apiserver_hostname = \"mycluster.example.com\"\n</code></pre> nyl-profile.yaml<pre><code>default:\n  kubeconfig:\n    type: ssh\n    user: root\n    host: mycluster.example.com\n    path: /etc/rancher/k3s/k3s.yaml\n    replace_apiserver_hostname: mycluster.example.com\n</code></pre> nyl-profile.json<pre><code>{\n  \"default\": {\n    \"kubeconfig\": {\n      \"type\": \"ssh\",\n      \"user\": \"root\",\n      \"host\": \"mycluster.example.com\",\n      \"path\": \"/etc/rancher/k3s/k3s.yaml\",\n      \"replace_apiserver_hostname\": \"mycluster.example.com\"\n    }\n}\n</code></pre> <p>If you are you are already setup with a kubeconfig file, you can specify the path to the file directly or have it automatically use your <code>~/.kube/config</code> file/<code>KUBECONFIG</code> environment variable. You may specify the context to use from that kubeconfig file which ensures Nyl interacts with the correct cluster, even if your <code>kubectl config get-contexts</code> indicates a different current context.</p> TOMLYAMLJSON nyl-profile.toml<pre><code>[default.kubeconfig]\ntype = \"local\"\ncontext = \"mycluster\"\n</code></pre> nyl-profile.yaml<pre><code>default:\n  kubeconfig:\n    type: local\n    context: mycluster\n</code></pre> nyl-profile.json<pre><code>{\n  \"default\": {\n    \"kubeconfig\": {\n      \"type\": \"local\",\n      \"context\": \"mycluster\"\n    }\n}\n</code></pre> <p>Implementation detail</p> <p>Nyl ensures that the correct context is used when interacting with the target cluster, e.g. when using <code>nyl run -- kubectl</code> or using <code>nyl profile activate</code> by generating a temporary kubeconfig file that is stripped down to include only the specified context. You can use <code>nyl profile get-kubeconfig</code> to retrieve the path of the temporary kubeconfig file.</p>"},{"location":"reference/configuration/profiles/#specification","title":"Specification","text":"<p>Todo</p> <p>Include specification of configuration data model.</p>"},{"location":"reference/configuration/profiles/#using-a-profile","title":"Using a profile","text":"<p>All Nyl commands that interact with the cluster will use the profile specified by the <code>NYL_PROFILE</code> environment variable or the one specified with the respective CLI option. If there is no profile configuration in your environment, Nyl will fall back to the global Kubernetes configuration file (equivalent of having a <code>default</code> profile with <code>type: local</code> and no <code>context</code> specified).</p> <p>You can update your shell by source-ing the output of the <code>nyl profile activate</code> command to set the <code>KUBECONFIG</code> and <code>KUBE_CONFIG_PATH</code> environment variables. (The latter is used for example for the Kubernetes Terraform provider).</p> <pre><code>$ nyl profile activate\nexport KUBECONFIG=/project/path/.nyl/profiles/default/kubeconfig.local\nexport KUBE_CONFIG_PATH=/project/path/.nyl/profiles/default/kubeconfig.local\n$ . &lt;(nyl profile activate)\n</code></pre> Tip: Using Direnv <p>When working in a project, you can use Direnv to automatically set the environment variables when you <code>cd</code> into a directory that contains configuration corresponding to a specific Kubernetes cluster.</p> .envrc<pre><code>export NYL_PROFILE=myprofile\n. &lt;(nyl profile activate)\n</code></pre> <p>If <code>NYL_PROFILE</code> is not set, Nyl will assume the default profile name is <code>default</code>.</p>"},{"location":"reference/configuration/profiles/#tunnel-management","title":"Tunnel management","text":"<p>The Nyl CLI will automatically manage tunnels to the target cluster by proxying through an SSH jump host.  The tunnel will typically remain open unless it is explicitly closed by the user to reduce the overhead of setting up the tunnel for each invocation of Nyl.</p> <p>Tunnels can be managed manually using the <code>nyl tun</code> command. Tunnel state is stored globally in <code>~/.nyl/tunnels/state.json</code>. Note that while you may have multiple <code>nyl-profiles.yaml</code> files on your system, the tunnel state is stored globally, and such is the interaction with <code>nyl tun</code>.</p> <pre><code>nyl tun status               List all known tunnels.\nnyl tun start &lt;profile&gt;      Open a tunnel to the cluster targeted by the profile.\nnyl tun stop [&lt;profile&gt;]     Close all tunnels or the tunnel for a specific profile.\n</code></pre>"},{"location":"reference/configuration/projects/","title":"Projects","text":"<p>A Nyl project is a collection of files that together describe a set of Kubernetes resources that are typically deployed to at least one Kubernetes cluster and source secrets from zero or more secrets provider. Kubernetes resources are defined in YAML files and may be templated using Nyl's structured templating or as Helm charts. Helm charts may be used as Nyl components.</p>"},{"location":"reference/configuration/projects/#configuration","title":"Configuration","text":"<p>Projects are defined in a <code>nyl-project.&lt;ext&gt;</code> file that is located in the current working directory or any of its parent directories. A project configuration file is not required to use Nyl, however it is recommended to set various project settings, such as the search path for Helm charts and Nyl components, whether to generate Nyl ApplySets, etc.</p> <p>A project configuration file may also contain the configuration for secrets providers and profiles, though if any configuration file closer to the current working directory for secrets providers or profiles is found, it will take precedence.</p>"},{"location":"reference/configuration/projects/#example","title":"Example","text":"<p>The following example demonstrates a simple project configuration file that sets the search path for Helm charts and Nyl components, enables the generation of ApplySets, and defines a default secrets provider.</p> nyl-project.toml<pre><code>[settings]\ngenerate_applysets = true\nsearch_path = [\"packages\"]\n\n[profiles.default.kubeconfig]\ntype = \"local\"\ncontext = \"my-cluster\"\n\n[secrets.default]\ntype = \"sops\"\npath = \"secrets.yaml\"\n</code></pre>"},{"location":"reference/configuration/projects/#project-structure","title":"Project structure","text":"<p>Nyl is not too opinionated about the project structure, but it was built with support for a certain structure in mind. The following is a suggestion for how to structure your project.</p>"},{"location":"reference/configuration/projects/#homogenous-targets","title":"Homogenous targets","text":"<p>With mostly homogenous clusters (e.g. referencing the same secrets, local helm charts, etc.), a typical project structure may have all Nyl configuration files at the top-level.</p> <p>If you're using ArgoCD, it's also common to further organize the cluster-specific configuration files in ArgoCD project-specific directories. We recommend to self-manage ArgoCD only from the <code>default</code> project.</p> <pre><code>clusters/\n\u2514\u2500\u2500 my-cluster/\n    \u251c\u2500\u2500 .envrc\n    \u251c\u2500\u2500 default/\n    \u2502   \u2514\u2500\u2500 argocd.yaml\n    \u2514\u2500\u2500 main-project/\n        \u2514\u2500\u2500 myapp.yaml\ncomponents/\nnyl-project.toml\n</code></pre> <p>Further reading</p> <ul> <li>Components</li> <li>ArgoCD ApplicationSet Example</li> </ul>"},{"location":"reference/configuration/projects/#heterogenous-targets","title":"Heterogenous targets","text":"<p>For more complex projects with multiple clusters that all look very different and reference differnt secrets, etc., you may want to move your Nyl configuration files closer to the cluster-specific configuration.</p> <pre><code>clusters/\n\u2514\u2500\u2500 main-cluster/\n\u2502   \u251c\u2500\u2500 .envrc\n\u2502   \u251c\u2500\u2500 default/\n\u2502   \u2502   \u2514\u2500\u2500 argocd.yaml\n\u2502   \u2514\u2500\u2500 project-a/\n\u2502       \u2514\u2500\u2500 myapp.yaml\n\u2514\u2500\u2500 my-other-cluster/\n    \u251c\u2500\u2500 .envrc\n    \u2514\u2500\u2500 project-b/\n        \u2514\u2500\u2500 myapp.yaml\nnyl-project.yaml\n</code></pre> <p>If you're using ARgoCD,  you can image <code>main-cluster</code> containing the ArgoCD instance that also deploys the <code>my-other-cluster</code>.</p>"},{"location":"reference/configuration/secrets/","title":"Secrets","text":"<p>You can connect Nyl with various secret providers to retrieve external or encrypted data that can be used in your templates. This is useful for keeping sensitive data out of your configuration files and ensuring that they are not accidentally committed to a version control system.</p>"},{"location":"reference/configuration/secrets/#configuration","title":"Configuration","text":"<p>Secret providers are configured in a <code>nyl-secrets.&lt;ext&gt;</code> file that is located in the current working directory or any of its parent directories. Secret providers may also be defined in a Project configuration file, though the file closer to the working directory will take precedence.</p> <p>There is no \"global\" way to define a secrets provider, as secrets are considered project-specific.</p> <p>As with other configuration file types, the file extension can be <code>.toml</code>, <code>.yaml</code>, or <code>.json</code>.</p> <p>The configuration contains any number of named secret providers. When not specified otherwise, Nyl will assume that the provider to use is named <code>default</code>. The provider to use can be overriden by passing the corresponding CLI option to respective Nyl commands or by setting the <code>NYL_SECRETS</code> environment variable.</p>"},{"location":"reference/configuration/secrets/#inspecting-secret-providers","title":"Inspecting secret providers","text":"<p>You can inspect secret providers using the <code>nyl secrets</code> command.</p> <pre><code>nyl secrets list            List the keys for all secrets in the provider.\nnyl secrets get &lt;key&gt;       Get the value of a secret as JSON.\n</code></pre>"},{"location":"reference/configuration/secrets/#templating","title":"Templating","text":"<p>Secrets are made available to templates using the <code>secrets.get()</code> function. The function takes a single argument, the key of the secret to retrieve.</p> <p>Example</p> <pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\ntype: Opaque\nstringData:\n  password: ${{ secrets.get(\"my-password\") }}\n</code></pre>"},{"location":"reference/configuration/secrets/#provider-sops","title":"Provider: Sops","text":"<p>Allows you to retrieve secrets from a Sops encrypted file. For a GitOps workflow, the file must be commited to the same repository to ensure that Nyl has access to it when it is invoked as an ArgoCD Config Management plugin. You also must have the <code>sops</code> program installed.</p> <p>Example</p> TOMLYAMLJSON nyl-secrets.toml<pre><code>[default]\ntype = \"sops\"\npath = \"../secrets.yaml\"\n</code></pre> nyl-secrets.yaml<pre><code>default:\n  type: sops\n  path: ../secrets.yaml\n</code></pre> nyl-secrets.json<pre><code>{\n  \"default\": {\n    \"type\": \"sops\",\n    \"path\": \"../secrets.yaml\"\n  }\n}\n</code></pre> <p>The secrets will be decoded using the <code>sops</code> program, hence all the typical ways to configure Sops and how it decrypts files apply. The <code>path</code> field is relative to the location of the <code>nyl-secrets.yaml</code> file.</p>"},{"location":"reference/configuration-api/profiles/","title":"Profiles API","text":""},{"location":"reference/configuration-api/profiles/#nyl.profiles.config","title":"<code>nyl.profiles.config</code>","text":""},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh","title":"<code>KubeconfigFromSsh</code>  <code>dataclass</code>","text":"<p>Represents how to obtain the Kubeconfig from an SSH connection.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.context","title":"<code>context: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The context to use from the Kubeconfig file. If not specified, the current context is used.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.host","title":"<code>host: str</code>  <code>instance-attribute</code>","text":"<p>The remote host to connect to.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.identity_file","title":"<code>identity_file: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An SSH private key file to use for authentication.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.path","title":"<code>path: str</code>  <code>instance-attribute</code>","text":"<p>The path where the Kubeconfig can be retrieved from.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.replace_apiserver_hostname","title":"<code>replace_apiserver_hostname: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Replace the hostname in the apiserver configuration of the Kubeconfig. This is useful for example with K3s when reading reading the <code>/etc/rancher/k3s/k3s.yaml</code> file from a remote host, but the API server in that file is not reachable from the local machine (e.g. because it <code>0.0.0.0</code>).</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.KubeconfigFromSsh.user","title":"<code>user: str</code>  <code>instance-attribute</code>","text":"<p>The username to connect to the remote host with.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.LocalKubeconfig","title":"<code>LocalKubeconfig</code>  <code>dataclass</code>","text":"<p>Use the local Kubeconfig file, either from the default location or a custom path specified in the environment.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.LocalKubeconfig.context","title":"<code>context: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The context to use from the Kubeconfig file. If not specified, the current context is used.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.LocalKubeconfig.path","title":"<code>path: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Path to the Kubernetes configuration file. Relative to the profile configuration file. If not specified, it falls back to the default location (per <code>KUBECONFIG</code> or otherwise <code>~/.kube/config</code>).</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.Profile","title":"<code>Profile</code>  <code>dataclass</code>","text":"<p>Represents a Kubernetes connection profile.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.Profile.kubeconfig","title":"<code>kubeconfig: LocalKubeconfig | KubeconfigFromSsh = field(default_factory=lambda: LocalKubeconfig())</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Describe how the Kubeconfig is to be obtained.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.Profile.tunnel","title":"<code>tunnel: SshTunnel | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>Describe how to create an SSH tunnel to reach the Kubernetes cluster API.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.ProfileConfig","title":"<code>ProfileConfig</code>  <code>dataclass</code>","text":""},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.ProfileConfig.load","title":"<code>load(file=None, /, *, cwd=None, required=True)</code>  <code>staticmethod</code>","text":"<p>Load the profiles configuration from the given file or the default file. If the configuration file does not exist, an error is raised unless required is set to <code>False</code>, in which case an empty configuration is returned.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.SshTunnel","title":"<code>SshTunnel</code>  <code>dataclass</code>","text":"<p>Configuration for an SSH tunnel.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.SshTunnel.host","title":"<code>host: str</code>  <code>instance-attribute</code>","text":"<p>The host to tunnel through.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.SshTunnel.identity_file","title":"<code>identity_file: str | None = None</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>An SSH private key file to use for authentication.</p>"},{"location":"reference/configuration-api/profiles/#nyl.profiles.config.SshTunnel.user","title":"<code>user: str</code>  <code>instance-attribute</code>","text":"<p>The username to connect to the remote host with.</p>"},{"location":"reference/configuration-api/secrets/","title":"Secrets API","text":""},{"location":"reference/configuration-api/secrets/#nyl.secrets.sops","title":"<code>nyl.secrets.sops</code>","text":""},{"location":"reference/configuration-api/secrets/#nyl.secrets.sops.SopsFile","title":"<code>SopsFile</code>  <code>dataclass</code>","text":"<p>               Bases: <code>SecretProvider</code></p> <p>This secrets provider decodes a SOPS-encrypted YAML or JSON file and serves the secrets stored within.</p> <p>Nested structures are supported, and the provider maps them to fully qualified keys using dot notation. The nested structure can be accessed as well, returning the full structure as a JSON object.</p>"},{"location":"reference/configuration-api/secrets/#nyl.secrets.sops.SopsFile.do_not_use_in_prod_only_for_testing_sops_age_key","title":"<code>do_not_use_in_prod_only_for_testing_sops_age_key: str | None = field(default=None, repr=False)</code>  <code>class-attribute</code> <code>instance-attribute</code>","text":"<p>The key to use for the <code>--age</code> option of SOPS. This is useful for testing purposes only and should not be used in production.</p>"},{"location":"reference/configuration-api/secrets/#nyl.secrets.sops.SopsFile.path","title":"<code>path: Path</code>  <code>instance-attribute</code>","text":"<p>The path to the SOPS-encrypted file. This path is resolved relative to the configuration file that the provider is defined in.</p>"},{"location":"reference/configuration-api/secrets/#nyl.secrets.sops.detect_sops_format","title":"<code>detect_sops_format(suffix)</code>","text":"<p>Tells the SOPS file format based on the file suffix. Never returns \"binary\". Returns <code>None</code> if the format cannot be determined.</p>"},{"location":"reference/templating/basics/","title":"Basics","text":"<p>Todo</p> <p>See structured-templates.</p>"},{"location":"reference/templating/components/","title":"Components","text":"<p>Nyl components are effectively templates that you can instantiate them similar to standard Kubernetes resources. They are treated in a way similar to CRDs, only that they will never to pushed to the Kubernetes API server and instead be replaced throuhg a first reconcilation phase during <code>nyl template</code>.</p> <p>Components are built on top of Helm chart inlining.</p>"},{"location":"reference/templating/components/#how-nyl-looks-for-components","title":"How Nyl looks for components","text":"<p>Nyl by default looks for components in a <code>components/</code> directory relative to your <code>nyl-project.ext</code> (or relative to your current working directory if there is no <code>nyl-project.&lt;ext&gt;</code>). You can also override the path where Nyl looks in the <code>nyl-project.&lt;ext&gt;</code>:</p> TOMLYAMLJSON nyl-project.toml<pre><code>[settings]\ncomponents_path = \"../../components\"\n</code></pre> nyl-project.yaml<pre><code>settings:\n  components_path: ../../components\n</code></pre> nyl-project.json<pre><code>{\n    \"settings\": {\n        \"components_path\": \"../../components\"\n    }\n}\n</code></pre>"},{"location":"reference/templating/components/#component-directory-structure","title":"Component directory structure","text":"<p>A component must exist in a directory relative to the <code>components_path</code> in a path formatted as <code>{apiVersion}/{kind}</code>. Hence, a typical project structure could look like this:</p> <pre><code>/\n    components/\n        example.org/\n            v1/\n                MyComponent/\n                    [component definition]\n    nyl-project.yaml\n    myapp.yaml\n</code></pre> <p>The <code>[component definition]</code> must be a type of component Nyl understands. Currently, it only supports Helm charts.</p>"},{"location":"reference/templating/components/#using-components","title":"Using components","text":"<p>In your application manifests, you can instantiate a component by declaring it similar to a standard Kubernetes resource. Nyl will try to lookup if that component exists and then instantiate it, or otherwise leave the resource untouched.</p> myapp.yaml<pre><code>apiVersion: example.org/v1\nkind: MyComponent\nmetadata:\n    name: mycomponent\nspec:\n    key: value\n</code></pre>"},{"location":"reference/templating/components/#component-resource-metadata","title":"Component resource metadata","text":"<p>The component's <code>metadata</code> field will be passed to the Helm values. This allows forwarding annotations and labels, if any.</p>"},{"location":"reference/templating/components/#tips-tricks","title":"Tips &amp; tricks","text":"<p>You can use <code>nyl new component example.org/v1 MyComponent</code> to create the boilerplate for a new Nyl component. Currently this is synonymous to calling <code>nyl new chart {components_path}/example.org/v1/MyComponent</code>.</p>"},{"location":"reference/templating/secrets/","title":"Secret injection","text":"<p>Danger</p> <p>When using secret injection with Nyl, you must make sure that you are aware of the risk profile for unintentionally revealing a secret in ArgoCD, which only masks out the data for actual Kubernetes <code>Secret</code> resources. Any other resource that contains the secret will be rendered in plain text.</p> <p>Secrets can be injected into your application configuration using the <code>secrets.get(key)</code> function. The <code>key</code> is the name of the secret as it is stored in the cluster. You can inspect all available keys of your configured secrets provider using the <code>nyl secrets list</code> command.</p>"},{"location":"reference/templating/secrets/#example","title":"Example","text":"<pre><code>apiVersion: v1\nkind: Secret\nmetadata:\n  name: my-secret\nstringData:\n  my-secret: ${{ secrets.get(\"my-secret\") }}\n</code></pre>"},{"location":"reference/templating/secrets/#syncing-secret-updates","title":"Syncing secret updates","text":"<p>When a secret is injected via Nyl and the secret is updated in the secrets provider, you must re-run <code>nyl template</code> and apply the updated configuration to the cluster. When using ArgoCD, this can happen automatically simply by re-syncing the application (or enabling auto-sync as the change to the secret value will be considered as drift to the desired configuration).</p> <p>Todo</p> <p>ArgoCD caches generated manifests so there may be a time delay between the secret update and ArgoCD fully re-materilizing the desired manifests with the updated secret being taken into account. What's the cache TTL, can it be changed/flushed?</p> <p>(A \"hard refresh\" usually works, but for automatic drift reconcilation when secrets update, having a lower TTL is important).</p>"},{"location":"reference/templating/inlining/helmcharts/","title":"HelmChart","text":"<p>Nyl allows you to inline Helm charts by specifying a <code>HelmChart</code> resource in your manifest. Just like any other resource, the Helm chart resource values can be templated using Nyl's structured templating (for example to inject secrets), and are then rendered to Kubernetes resources that are inlined in your configuration.</p> <p>Example</p> <pre><code>apiVersion: inline.nyl.io/v1\nkind: HelmChart\nmetadata:\n  name: my-release\n  namespace: default\nspec:\n  chart:\n    repository: https://charts.bitnami.com/bitnami\n    name: nginx\n    # version: \"...\"\n  values:\n    controller:\n      service:\n        type: LoadBalancer\n</code></pre>"},{"location":"reference/templating/inlining/helmcharts/#comparison-to-native-argocd-helm-applications","title":"Comparison to native ArgoCD Helm applications","text":"<p>Nyl may look similar to Helm in the sense that it allows for templating YAML files. However, there are some important differences between the two that make Nyl the better choice for defining applications in a GitOps repository.</p>"},{"location":"reference/templating/inlining/helmcharts/#passing-secrets-more-safely","title":"Passing secrets more safely","text":"<p>Values for ArgoCD Helm applications are either stored in a values-file in the repository (in plain-text) or in the <code>valuesObject</code> configuration of the ArgoCD <code>Application</code> spec (again, in plain-text). The <code>valuesObject</code> like standard Kubernetes <code>Secret</code> resources when inspected via the ArgoCD UI, which makes storing the secrets are part of the application spec undesirable.</p> <p>Danger</p> <p>You must still be careful when using Nyl to inject secrets into Helm charts. The Helm chart may pass the secret value into a resource that is not a <code>Secret</code>, which would then be rendered in plain-text in the cluster when inspected in ArgoCD or via <code>kubectl</code>.</p>"},{"location":"reference/templating/inlining/helmcharts/#combining-multiple-helm-charts","title":"Combining multiple Helm charts","text":"<p>An ArgoCD application supports only a single Helm chart. If you need to deploy multiple Helm charts as part of a single application, you would need to create a Helm chart that includes all the other charts. However, this can lead to a complicated setup that is hard to maintain: It either requires you to repeat the same values in multiple places, or all subcharts support <code>globals</code>.</p>"},{"location":"reference/templating/inlining/helmcharts/#secret-injection","title":"Secret injection","text":"<p>Natively, ArgoCD applications do not support injecting secrets into the Helm chart values. With Nyl, you can connect to a secrets provider and inject secrets into the generated resources or the value of a Helm chart parameter. Your YAML template becomes the glue code for propagating secrets from the point of origin into your Kubernetes cluster and application.</p> <p>In many cases you can work around this limitation by placing a <code>Secret</code> resource into your cluster, either manually or by other means (such as using external-secrets), but this does not cover the use case for Helm charts that require a secret, or more generally, an external parameter, in a place where an existing secret cannot be configured (e.g. either because the chart simply does not support it or because it needs to be in a specific place/format). This is most commonly an issue when deploying third-party applications from Helm charts.</p>"},{"location":"reference/templating/inlining/helmcharts/#pipelining-between-applications-todo","title":"Pipelining between applications (TODO)","text":"<p>Nyl supports looking up information in the cluster at time of rendering the resources. This allows for iteratively reconciling resources in the cluster that depend on each other. For example, it is not uncommon to have an application generate a <code>Secret</code> that later needs to be transformed and piped into another Helm chart.</p> <p>Danger</p> <p>When this feature is enabled, Nyl would allow lookups across the entire cluster (or the resources that the ArgoCD service account has access to). This is a powerful feature that can be used to build complex applications, but it also comes with a security risk when a cluster is shared between multiple teams.</p> <p>Todo</p> <p>Explain how this feature works and how to enable it.</p>"},{"location":"reference/templating/inlining/overview/","title":"Inlining Overview","text":"<p>Nyl supports inlining of Kubernetes resources generated from templates in your configuration. Resources that are inlined by Nyl are typically in the <code>inline.nyl.io/v1</code> API group, however Nyl also has the concept of components which effectively allow you to define your own API groups and resource kinds for inlined resources.</p>"},{"location":"reference/templating/inlining/postprocessor/","title":"PostProcessor","text":"<p>The <code>inline.nyl.io/v1/PostProcessor</code> resource can be used to configure post-processing steps for all Kubernetes resources defined in the same Manifest as the <code>PostProcessor</code>. This resource does not accept a <code>metadata</code> field.</p>"},{"location":"reference/templating/inlining/postprocessor/#prerequisites","title":"Prerequisites","text":"<ul> <li>You need the <code>kyverno</code> CLI installed </li> </ul>"},{"location":"reference/templating/inlining/postprocessor/#api-spec","title":"API Spec","text":"<pre><code>apiVersion: inline.nyl.io/v1\nkind: PostProcessor\nspec:\n  kyverno:\n    # A list of files that each contain a Kyverno policy resource, usually a `ClusterPolicy`. The paths are first\n    # considered relative to the manifest that this resource is defined in, and will then be searched in the project\n    # search path.\n    policyFiles:\n    - path/to/policy.yaml\n\n    # A mapping of policy names (what you would give as a filename) and a YAML document that represents the Kyverno\n    # policy resource (usually a `ClusterPolicy`).\n    inlinePolicies:\n      my-policy:\n        apiVersion: kyverno.io/v1\n        kind: ClusterPolicy\n        metadata:\n          name: enforce-pod-security-context\n        spec: {} # ...\n</code></pre>"},{"location":"reference/templating/inlining/postprocessor/#example","title":"Example","text":"<p>If you're deploying to hardened RKE2, your pods must have a specific <code>securityContext</code> configuration in order to be allowed by the PodSecurityPolicy. The application's Helm charts that you deploy may have options to inject the required options, but if they are not you're usually out of luck unless you fork the Helm chart, or materialize the resources and edit them in your project.</p> <p>With the Nyl <code>PostProcessor</code>, you can apply Kyverno policies to validate or mutate the resources in a manifest.</p> ManifestKyverno Policy forgejo.yaml<pre><code>---\napiVersion: inline.nyl.io/v1\nkind: PostProcessor\nspec:\n  kyverno:\n    policyFiles:\n    - ./policies/security-profile.yaml\n\n---\napiVersion: v1\nkind: Namespace\nmetadata:\n  name: forgejo\n\n---\napiVersion: inline.nyl.io/v1\nkind: HelmChart\nmetadata:\n  name: forgejo\n  namespace: forgejo\nspec:\n  chart:\n    repository: oci://code.forgejo.org/forgejo-helm # https://artifacthub.io/packages/helm/forgejo-helm/forgejo\n    name: forgejo\n    version: \"10.0.1\"\n  values: {}\n</code></pre> policies/security-profile.yaml<pre><code>apiVersion: kyverno.io/v1\nkind: ClusterPolicy\nmetadata:\n  name: enforce-security-context\nspec:\n  validationFailureAction: enforce\n  rules:\n    - name: mutate-pod-security-context\n      match:\n        resources:\n          kinds:\n            - Pod\n      mutate:\n        patchStrategicMerge: &amp;podSpec\n          spec:\n            securityContext:\n              runAsNonRoot: true\n              seccompProfile:\n                type: RuntimeDefault\n            containers: &amp;containers\n              - (name): \"*\"\n                securityContext:\n                  runAsNonRoot: true\n                  allowPrivilegeEscalation: false\n                  capabilities:\n                    drop:\n                      - \"ALL\"\n            initContainers: *containers\n    - name: mutate-deployment-security-context\n      match:\n        resources:\n          kinds:\n            - Deployment\n      mutate:\n        patchStrategicMerge:\n          spec:\n            template: *podSpec\n</code></pre> <p>Running <code>nyl template forgejo.yaml</code> will use the <code>kyverno</code> CLI to apply the policy to the manifests generated by the Helm chart. Note that the post processing happens at the very end after all other Kubernetes manifests have been generated.</p>"}]}